{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "YOU MUST!!!\n",
    "Read all the lines of the code provided to you and understand what it does!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DecisionNode(object):\n",
    "    \"\"\"\n",
    "    README\n",
    "    DecisionNode is a building block for Decision Trees.\n",
    "    DecisionNode is a python class representing a  node in our decision tree\n",
    "    node = DecisionNode()  is a simple usecase for the class\n",
    "    you can also initialize the class like this:\n",
    "    node = DecisionNode(column = 3, value = \"Car\")\n",
    "    In python, when you initialize a class like this, its __init__ method is called \n",
    "    with the given arguments. __init__() creates a new object of the class type, and initializes its \n",
    "    instance attributes/variables.\n",
    "    In python the first argument of any method in a class is 'self'\n",
    "    Self points to the object which it is called from and corresponds to 'this' from Java\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 column=None,\n",
    "                 value=None,\n",
    "                 false_branch=None,\n",
    "                 true_branch=None,\n",
    "                 current_results=None,\n",
    "                 is_leaf=False,\n",
    "                 results=None):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.false_branch = false_branch\n",
    "        self.true_branch = true_branch\n",
    "        self.current_results = current_results\n",
    "        self.is_leaf = is_leaf\n",
    "        self.results=results\n",
    "\n",
    "def dict_of_values(data):\n",
    "    \"\"\"\n",
    "    param data: a 2D Python list representing the data. Last column of data is Y.\n",
    "    return: returns a python dictionary showing how many times each value appears in Y\n",
    "\n",
    "    for example \n",
    "    data = [[1,'yes'],[1,'no'],[1,'yes'],[1,'yes']]\n",
    "    dict_of_values(data)\n",
    "    should return {'yes' : 3, 'no' :1}\n",
    "    \"\"\"\n",
    "    results = defaultdict(int)\n",
    "    for row in data:\n",
    "        r = row[-1]\n",
    "        results[r] += 1\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "def divide_data(data, feature_column, feature_val):\n",
    "    \"\"\"\n",
    "    this function dakes the data and divides it in two parts by a line. A line\n",
    "    is defined by the feature we are considering (feature_column) and the target \n",
    "    value. The function returns a tuple (data1, data2) which are the desired parts of the data.\n",
    "    For int or float types of the value, data1 have all the data with values >= feature_val\n",
    "    in the corresponding column and data2 should have rest.\n",
    "    For string types, data1 should have all data with values == feature val and data2 should \n",
    "    have the rest.\n",
    "\n",
    "    param data: a 2D Python list representing the data. Last column of data is Y.\n",
    "    param feature_column: an integer index of the feature/column.\n",
    "    param feature_val: can be int, float, or string\n",
    "    return: a tuple of two 2D python lists\n",
    "    \"\"\"\n",
    "    if type(feature_val) == int or type(feature_val) == float:\n",
    "        data1=[e for e in data if e[feature_column]>=feature_val]\n",
    "        data2=[e for e in data if e[feature_column]<feature_val]\n",
    "        pass\n",
    "    else:\n",
    "        data1=[e for e in data if e[feature_column]==feature_val]\n",
    "        data2=[e for e in data if e[feature_column]!=feature_val]\n",
    "        pass\n",
    "    return data1, data2\n",
    "\n",
    "\n",
    "def gini_impurity(data1, data2):\n",
    "\n",
    "    \"\"\"\n",
    "    Given two 2D lists of compute their gini_impurity index. \n",
    "    Remember that last column of the data lists is the Y\n",
    "    Lets assume y1 is y of data1 and y2 is y of data2.\n",
    "    gini_impurity shows how diverse the values in y1 and y2 are.\n",
    "    gini impurity is given by \n",
    "\n",
    "    N1*sum(p_k1 * (1-p_k1)) + N2*sum(p_k2 * (1-p_k2))\n",
    "\n",
    "    where N1 is number of points in data1\n",
    "    p_k1 is fraction of points that have y value of k in data1\n",
    "    same for N2 and p_k2\n",
    "\n",
    "\n",
    "    param data1: A 2D python list\n",
    "    param data2: A 2D python list\n",
    "    return: a number - gini_impurity \n",
    "    \"\"\"\n",
    "\n",
    "    results1 = defaultdict(int)\n",
    "    a1=[]\n",
    "    l1=0\n",
    "    for row in data1:\n",
    "        r = row[len(row) - 1]\n",
    "        if results1[r]==0:\n",
    "            a1.append(r)\n",
    "        results1[r] += 1\n",
    "        l1+=1\n",
    "    results2 = defaultdict(int)\n",
    "    a2=[]\n",
    "    l2=0\n",
    "    for row in data2:\n",
    "        r = row[len(row) - 1]\n",
    "        if results2[r]==0:\n",
    "            a2.append(r)\n",
    "        results2[r] += 1\n",
    "        l2+=1\n",
    "    s1=0\n",
    "    for i in a1:\n",
    "        s1+=results1[i]*(1-results1[i]/l1)\n",
    "    s2=0\n",
    "    for i in a2:\n",
    "        s2+=results2[i]*(1-results2[i]/l2)\n",
    "    return s1 + s2\n",
    "\n",
    "\n",
    "def build_tree(data, current_depth=0, max_depth=1e10):\n",
    "    \"\"\"\n",
    "    build_tree is a recursive function.\n",
    "    What it does in the general case is:\n",
    "    1: find the best feature and value of the feature to divide the data into\n",
    "    two parts\n",
    "    2: divide data into two parts with best feature, say data1 and data2\n",
    "        recursively call build_tree on data1 and data2. this should give as two \n",
    "        trees say t1 and t2. Then the resulting tree should be \n",
    "        DecisionNode(...... true_branch=t1, false_branch=t2) \n",
    "\n",
    "\n",
    "    In case all the points in the data have same Y we should not split any more, and return that node\n",
    "    For this function we will give you some of the code so its not too hard for you ;)\n",
    "    \n",
    "    param data: param data: A 2D python list\n",
    "    param current_depth: an integer. This is used if we want to limit the numbr of layers in the\n",
    "        tree\n",
    "    param max_depth: an integer - the maximal depth of the representing\n",
    "    return: an object of class DecisionNode\n",
    "\n",
    "    \"\"\"\n",
    "    if(current_depth == max_depth):\n",
    "        return DecisionNode(current_results=dict_of_values(data),is_leaf=True)\n",
    "\n",
    "    if(len(dict_of_values(data)) == 1):\n",
    "        return DecisionNode(current_results=dict_of_values(data), is_leaf=True)\n",
    "\n",
    "    #This calculates gini number for the data before dividing \n",
    "    self_gini = gini_impurity(data, [])\n",
    "\n",
    "    #Below are the attributes of the best division that you need to find. \n",
    "    #You need to update these when you find a division which is better\n",
    "    best_gini = 1e10\n",
    "    best_column = None\n",
    "    best_value = None\n",
    "    #best_split is tuple (data1,data2) which shows the two datas for the best divison so far\n",
    "    best_split = None\n",
    "    for i in data:\n",
    "        for j in range(len(i)-1):\n",
    "            d1,d2=divide_data(data,j,i[j])\n",
    "            gini=gini_impurity(d1,d2)\n",
    "            if gini<best_gini:\n",
    "                best_gini=gini\n",
    "                best_column=j\n",
    "                best_value=i[j]\n",
    "                best_split=(d1,d2)\n",
    "    #You need to find the best feature to divide the data\n",
    "    #For each feature and each possible value of the feature compute the \n",
    "    # gini number for that division. You need to find the feature that minimizes\n",
    "    # gini number. Remember that last column of data is Y\n",
    "    # Think how you can use the divide_data and gini_impurity functions you wrote \n",
    "    # above\n",
    "    \n",
    "\n",
    "    #if best_gini is no improvement from self_gini, we stop and return a node.\n",
    "    if abs(self_gini - best_gini) < 1e-10:\n",
    "        return DecisionNode(current_results=dict_of_values(data), is_leaf=True)\n",
    "    else:\n",
    "        #recursively call build tree, construct the correct return argument and return\n",
    "        return DecisionNode( #<---- FIX THIS\n",
    "            column=best_column,\n",
    "            value=best_value,\n",
    "            is_leaf=False,\n",
    "            false_branch=build_tree(best_split[1],current_depth+1,max_depth),\n",
    "            true_branch=build_tree(best_split[0],current_depth+1,max_depth),\n",
    "            current_results=dict_of_values(data)\n",
    "        )\n",
    "\n",
    "\n",
    "def print_tree(tree, indent=''):\n",
    "    # Is this a leaf node?\n",
    "    if tree.is_leaf:\n",
    "        print(str(tree.current_results))\n",
    "    else:\n",
    "        # Print the criteria\n",
    "        #         print (indent+'Current Results: ' + str(tree.current_results))\n",
    "        print('Column ' + str(tree.column) + ' : ' + str(tree.value) + '? ')\n",
    "\n",
    "        # Print the branches\n",
    "        print(indent + 'True->')\n",
    "        print_tree(tree.true_branch, indent + '  ')\n",
    "        print(indent + 'False->')\n",
    "        print_tree(tree.false_branch, indent + '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"\n",
    "    DecisionTree class, that represents one Decision Tree\n",
    "\n",
    "    :param max_tree_depth: maximum depth for this tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tree_depth):\n",
    "        self.max_depth = max_tree_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        if not isinstance(X,list):\n",
    "            X = X.tolist()\n",
    "        if not isinstance(Y,list):\n",
    "            Y= Y.tolist()\n",
    "        data=[x+y for x,y in zip(X,Y)]\n",
    "        self.trees=build_tree(data,0,self.max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "        Y=[]\n",
    "        for x in X:\n",
    "            tree=self.trees\n",
    "            while not tree.is_leaf:\n",
    "                if x[tree.column] >= tree.value:\n",
    "                    tree=tree.true_branch\n",
    "                else:\n",
    "                    tree=tree.false_branch\n",
    "            Y.append(max(tree.current_results,key=tree.current_results.get))\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class RandomForest(object):\n",
    "    \"\"\"\n",
    "    RandomForest a class, that represents Random Forests.\n",
    "\n",
    "    :param num_trees: Number of trees in the random forest\n",
    "    :param max_tree_depth: maximum depth for each of the trees in the forest.\n",
    "    :param ratio_per_tree: ratio of points to use to train each of\n",
    "        the trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_trees, max_tree_depth, ratio_per_tree=0.5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.ratio_per_tree=ratio_per_tree\n",
    "        self.trees = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.num_trees):\n",
    "            indices=np.arange(Y.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X=X[indices]\n",
    "            Y=Y[indices]\n",
    "            test_len=int(self.ratio_per_tree*len(X))\n",
    "            X_train=X[:test_len]\n",
    "            Y_train=Y[:test_len]\n",
    "            tree=DecisionTree(self.max_tree_depth)\n",
    "            tree.fit(X_train,Y_train)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: (Y, conf), tuple with Y being 1 dimension python\n",
    "        list with labels, and conf being 1 dimensional list with\n",
    "        confidences for each of the labels.\n",
    "        \"\"\"\n",
    "        predicts=[tree.predict(X) for tree in self.trees]\n",
    "        predicts=[[predicts[i][j] for i in range(len(predicts))] for j in range(len(predicts[0]))]\n",
    "        Y=[]\n",
    "        conf=[]\n",
    "        for r in predicts:\n",
    "            results = defaultdict(np.float64)\n",
    "            count=0\n",
    "            for key in r:\n",
    "                results[key]+=1\n",
    "                count+=1\n",
    "            Y.append(max(results,key=results.get))\n",
    "            conf.append(results[Y[-1]]/count)\n",
    "        return (Y, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(s):\n",
    "    return 1 - 1 / (1 + np.exp(s)) if abs(s) < 10 else 0 if s < 0 else 1\n",
    "\n",
    "\n",
    "def normalized_gradient(X, Y,beta,l):\n",
    "    \"\"\"\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param beta: value of beta (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :return: normalized gradient, i.e. gradient normalized according to data\n",
    "    \"\"\"\n",
    "    gr = l * beta\n",
    "    s = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        s += X[i] * Y[i] * (1 - sigmoid(Y[i] * beta.T.dot(X[i])))\n",
    "    gr -= s\n",
    "    s = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        s += Y[i] * (1 - sigmoid(Y[i] * beta.T.dot(X[i])))\n",
    "    gr[0] -= s\n",
    "    return gr / X.shape[0]\n",
    "\n",
    "\n",
    "def gradient_descent(xx, yy,l,epsilon,max_steps,step_size,beta):\n",
    "    \"\"\"\n",
    "    Implement gradient descent using full value of the gradient.\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :param epsilon: approximation strength\n",
    "    :param max_steps: maximum number of iterations before algorithm will\n",
    "        terminate.\n",
    "    :return: value of beta (1 dimensional np.array)\n",
    "    \"\"\"\n",
    "    X = xx.copy()\n",
    "    Y = yy.copy()\n",
    "    gr = np.zeros(X.shape[1])\n",
    "\n",
    "    v = np.var(X, axis=0, dtype=float) ** 0.5\n",
    "    m = np.mean(X, axis=0, dtype=float)\n",
    "    for i in range(1, X.shape[1]):\n",
    "        for j in range(X.shape[0]):\n",
    "            if v[i] != 0:\n",
    "                X[j][i] -= m[i]\n",
    "                X[j][i] /= v[i]\n",
    "    l = np.array([l / v[_] ** 2 if v[_] != 0 else 0 for _ in range(X.shape[1])])\n",
    "\n",
    "    for s in range(max_steps):\n",
    "        old = beta.copy()\n",
    "        gr = step_size * normalized_gradient(X, Y,beta,l)\n",
    "        beta -= gr\n",
    "        if ((beta - old) ** 2).sum() / ((beta ** 2).sum()) < epsilon ** 2:\n",
    "            break\n",
    "        pass\n",
    "    for i in range(X.shape[1]):\n",
    "        if i == 0:\n",
    "            beta[0] = beta[0] - np.sum(\n",
    "                np.array([m[i] * beta[i] / v[i] if v[i] != 0 else 0 for i in range(1, m.shape[0])]))\n",
    "        else:\n",
    "            beta[i] = beta[i] / v[i] if v[i] != 0 else 0\n",
    "    return beta\n",
    "\n",
    "\n",
    "class logistic(object):\n",
    "    def __init__(self,epsilon=1e-6, l=1, step_size=1e-4, max_steps=1000):\n",
    "        self.epsilon=epsilon\n",
    "        self.step_size=step_size\n",
    "        self.max_steps=max_steps\n",
    "        self.l=l\n",
    "    def fit(self,xx,yy):\n",
    "        \"\"\"\n",
    "        :param xx: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param yy: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        self.beta=np.random.normal(0,1,xx.shape[1])\n",
    "        self.beta=gradient_descent(xx,yy,epsilon=self.epsilon,l=self.l,step_size=self.step_size,max_steps=self.max_steps,beta=self.beta)\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: (Y, conf), tuple with Y being 1 dimension python\n",
    "        list with labels, and conf being 1 dimensional list with\n",
    "        confidences for each of the labels.\n",
    "        \"\"\"\n",
    "        return [1 if self.beta.dot(x)>0 else 0 for x in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter num :0\n",
      "\tDecision Tree Accuracy =  0.769230769231  ( 0.0 )\n",
      "\tRandom Forest Tree Accuracy =  0.769230769231  ( 0.0 )\n",
      "\tLogistic Reg. Accuracy =  0.730769230769  ( 0.0 )\n",
      "iter num :1\n",
      "\tDecision Tree Accuracy =  0.769230769231  ( 0.0 )\n",
      "\tRandom Forest Tree Accuracy =  0.807692307692  ( 0.0384615384615 )\n",
      "\tLogistic Reg. Accuracy =  0.730769230769  ( 0.0 )\n",
      "iter num :2\n",
      "\tDecision Tree Accuracy =  0.820512820513  ( 0.0725237724294 )\n",
      "\tRandom Forest Tree Accuracy =  0.820512820513  ( 0.0362618862147 )\n",
      "\tLogistic Reg. Accuracy =  0.794871794872  ( 0.0906547155367 )\n",
      "iter num :3\n",
      "\tDecision Tree Accuracy =  0.817307692308  ( 0.0630522935029 )\n",
      "\tRandom Forest Tree Accuracy =  0.798076923077  ( 0.0499630040645 )\n",
      "\tLogistic Reg. Accuracy =  0.788461538462  ( 0.0792904928003 )\n",
      "iter num :4\n",
      "\tDecision Tree Accuracy =  0.807692307692  ( 0.0595843591724 )\n",
      "\tRandom Forest Tree Accuracy =  0.8  ( 0.0448534761142 )\n",
      "\tLogistic Reg. Accuracy =  0.784615384615  ( 0.0713355268884 )\n",
      "iter num :5\n",
      "\tDecision Tree Accuracy =  0.794871794872  ( 0.0614850195297 )\n",
      "\tRandom Forest Tree Accuracy =  0.807692307692  ( 0.0444115591684 )\n",
      "\tLogistic Reg. Accuracy =  0.788461538462  ( 0.0656855818331 )\n",
      "iter num :6\n",
      "\tDecision Tree Accuracy =  0.802197802198  ( 0.0596856070945 )\n",
      "\tRandom Forest Tree Accuracy =  0.818681318681  ( 0.0491443511538 )\n",
      "\tLogistic Reg. Accuracy =  0.791208791209  ( 0.0611842237674 )\n",
      "iter num :7\n",
      "\tDecision Tree Accuracy =  0.798076923077  ( 0.0568853825298 )\n",
      "\tRandom Forest Tree Accuracy =  0.817307692308  ( 0.0461137646472 )\n",
      "\tLogistic Reg. Accuracy =  0.788461538462  ( 0.0576923076923 )\n",
      "iter num :8\n",
      "\tDecision Tree Accuracy =  0.803418803419  ( 0.0557196786769 )\n",
      "\tRandom Forest Tree Accuracy =  0.816239316239  ( 0.043581363364 )\n",
      "\tLogistic Reg. Accuracy =  0.794871794872  ( 0.0573350763461 )\n",
      "iter num :9\n",
      "\tDecision Tree Accuracy =  0.8  ( 0.0538461538462 )\n",
      "\tRandom Forest Tree Accuracy =  0.815384615385  ( 0.0414243446703 )\n",
      "\tLogistic Reg. Accuracy =  0.807692307692  ( 0.0666173387526 )\n",
      "iter num :10\n",
      "\tDecision Tree Accuracy =  0.797202797203  ( 0.0520967287614 )\n",
      "\tRandom Forest Tree Accuracy =  0.804195804196  ( 0.0530271010074 )\n",
      "\tLogistic Reg. Accuracy =  0.807692307692  ( 0.0635171402958 )\n",
      "iter num :11\n",
      "\tDecision Tree Accuracy =  0.791666666667  ( 0.053151038307 )\n",
      "\tRandom Forest Tree Accuracy =  0.807692307692  ( 0.0520771692605 )\n",
      "\tLogistic Reg. Accuracy =  0.810897435897  ( 0.0617351291162 )\n",
      "iter num :12\n",
      "\tDecision Tree Accuracy =  0.772189349112  ( 0.0846174535271 )\n",
      "\tRandom Forest Tree Accuracy =  0.798816568047  ( 0.0587261338499 )\n",
      "\tLogistic Reg. Accuracy =  0.798816568047  ( 0.0725907937656 )\n",
      "iter num :13\n",
      "\tDecision Tree Accuracy =  0.763736263736  ( 0.0870493380096 )\n",
      "\tRandom Forest Tree Accuracy =  0.791208791209  ( 0.0628874897926 )\n",
      "\tLogistic Reg. Accuracy =  0.796703296703  ( 0.0703640026092 )\n",
      "iter num :14\n",
      "\tDecision Tree Accuracy =  0.758974358974  ( 0.0859643826371 )\n",
      "\tRandom Forest Tree Accuracy =  0.784615384615  ( 0.0655728812951 )\n",
      "\tLogistic Reg. Accuracy =  0.789743589744  ( 0.0727952285466 )\n",
      "iter num :15\n",
      "\tDecision Tree Accuracy =  0.766826923077  ( 0.0886168682268 )\n",
      "\tRandom Forest Tree Accuracy =  0.786057692308  ( 0.0637359306902 )\n",
      "\tLogistic Reg. Accuracy =  0.793269230769  ( 0.0717941563609 )\n",
      "iter num :16\n",
      "\tDecision Tree Accuracy =  0.764705882353  ( 0.0863886112762 )\n",
      "\tRandom Forest Tree Accuracy =  0.787330316742  ( 0.0620421230806 )\n",
      "\tLogistic Reg. Accuracy =  0.796380090498  ( 0.0707533745299 )\n",
      "iter num :17\n",
      "\tDecision Tree Accuracy =  0.762820512821  ( 0.0843137592177 )\n",
      "\tRandom Forest Tree Accuracy =  0.786324786325  ( 0.0604364770245 )\n",
      "\tLogistic Reg. Accuracy =  0.797008547009  ( 0.0688087272996 )\n",
      "iter num :18\n",
      "\tDecision Tree Accuracy =  0.765182186235  ( 0.082674404278 )\n",
      "\tRandom Forest Tree Accuracy =  0.783400809717  ( 0.0601183903033 )\n",
      "\tLogistic Reg. Accuracy =  0.797570850202  ( 0.0670159731063 )\n",
      "iter num :19\n",
      "\tDecision Tree Accuracy =  0.763461538462  ( 0.0809293285096 )\n",
      "\tRandom Forest Tree Accuracy =  0.778846153846  ( 0.0618681074698 )\n",
      "\tLogistic Reg. Accuracy =  0.798076923077  ( 0.0653563287229 )\n",
      "iter num :20\n",
      "\tDecision Tree Accuracy =  0.767399267399  ( 0.0809183957303 )\n",
      "\tRandom Forest Tree Accuracy =  0.778388278388  ( 0.0604118040384 )\n",
      "\tLogistic Reg. Accuracy =  0.798534798535  ( 0.0638141076564 )\n",
      "iter num :21\n",
      "\tDecision Tree Accuracy =  0.770979020979  ( 0.0807419806906 )\n",
      "\tRandom Forest Tree Accuracy =  0.783216783217  ( 0.0630341132074 )\n",
      "\tLogistic Reg. Accuracy =  0.800699300699  ( 0.0631310142842 )\n",
      "iter num :22\n",
      "\tDecision Tree Accuracy =  0.769230769231  ( 0.0793918225449 )\n",
      "\tRandom Forest Tree Accuracy =  0.780936454849  ( 0.062569521518 )\n",
      "\tLogistic Reg. Accuracy =  0.797658862876  ( 0.0633688806739 )\n",
      "iter num :23\n",
      "\tDecision Tree Accuracy =  0.766025641026  ( 0.079225686502 )\n",
      "\tRandom Forest Tree Accuracy =  0.780448717949  ( 0.0612967676247 )\n",
      "\tLogistic Reg. Accuracy =  0.798076923077  ( 0.0620670408046 )\n",
      "iter num :24\n",
      "\tDecision Tree Accuracy =  0.770769230769  ( 0.0810288866601 )\n",
      "\tRandom Forest Tree Accuracy =  0.784615384615  ( 0.0634323942403 )\n",
      "\tLogistic Reg. Accuracy =  0.801538461538  ( 0.0631331831652 )\n",
      "iter num :25\n",
      "\tDecision Tree Accuracy =  0.772189349112  ( 0.0797720045131 )\n",
      "\tRandom Forest Tree Accuracy =  0.788461538462  ( 0.0651055653392 )\n",
      "\tLogistic Reg. Accuracy =  0.804733727811  ( 0.0639354520266 )\n",
      "iter num :26\n",
      "\tDecision Tree Accuracy =  0.767806267806  ( 0.0814087296993 )\n",
      "\tRandom Forest Tree Accuracy =  0.790598290598  ( 0.0648109012312 )\n",
      "\tLogistic Reg. Accuracy =  0.804843304843  ( 0.062742779332 )\n",
      "iter num :27\n",
      "\tDecision Tree Accuracy =  0.766483516484  ( 0.0802367135523 )\n",
      "\tRandom Forest Tree Accuracy =  0.793956043956  ( 0.0659912755465 )\n",
      "\tLogistic Reg. Accuracy =  0.803571428571  ( 0.0619656240035 )\n",
      "iter num :28\n",
      "\tDecision Tree Accuracy =  0.770557029178  ( 0.0817346362766 )\n",
      "\tRandom Forest Tree Accuracy =  0.79575596817  ( 0.0655392525231 )\n",
      "\tLogistic Reg. Accuracy =  0.803713527851  ( 0.0608925213965 )\n",
      "iter num :29\n",
      "\tDecision Tree Accuracy =  0.765384615385  ( 0.0850513245673 )\n",
      "\tRandom Forest Tree Accuracy =  0.79358974359  ( 0.0654850912263 )\n",
      "\tLogistic Reg. Accuracy =  0.803846153846  ( 0.0598733047633 )\n",
      "iter num :30\n",
      "\tDecision Tree Accuracy =  0.760545905707  ( 0.0877654550066 )\n",
      "\tRandom Forest Tree Accuracy =  0.792803970223  ( 0.0645638304281 )\n",
      "\tLogistic Reg. Accuracy =  0.803970223325  ( 0.0589036102753 )\n",
      "iter num :31\n",
      "\tDecision Tree Accuracy =  0.762019230769  ( 0.0867718540759 )\n",
      "\tRandom Forest Tree Accuracy =  0.790865384615  ( 0.0644571522971 )\n",
      "\tLogistic Reg. Accuracy =  0.808894230769  ( 0.0641313537113 )\n",
      "iter num :32\n",
      "\tDecision Tree Accuracy =  0.764568764569  ( 0.086655619093 )\n",
      "\tRandom Forest Tree Accuracy =  0.792540792541  ( 0.0641766895009 )\n",
      "\tLogistic Reg. Accuracy =  0.812354312354  ( 0.0661158722782 )\n",
      "iter num :33\n",
      "\tDecision Tree Accuracy =  0.763574660633  ( 0.0855625479518 )\n",
      "\tRandom Forest Tree Accuracy =  0.796380090498  ( 0.0669622108007 )\n",
      "\tLogistic Reg. Accuracy =  0.814479638009  ( 0.0662706720175 )\n",
      "iter num :34\n",
      "\tDecision Tree Accuracy =  0.764835164835  ( 0.0846510557036 )\n",
      "\tRandom Forest Tree Accuracy =  0.794505494505  ( 0.0668977198032 )\n",
      "\tLogistic Reg. Accuracy =  0.814285714286  ( 0.0653268744954 )\n",
      "iter num :35\n",
      "\tDecision Tree Accuracy =  0.764957264957  ( 0.083470192242 )\n",
      "\tRandom Forest Tree Accuracy =  0.791666666667  ( 0.0680665403256 )\n",
      "\tLogistic Reg. Accuracy =  0.813034188034  ( 0.0648373133363 )\n",
      "iter num :36\n",
      "\tDecision Tree Accuracy =  0.769230769231  ( 0.0862347407566 )\n",
      "\tRandom Forest Tree Accuracy =  0.7920997921  ( 0.067190696928 )\n",
      "\tLogistic Reg. Accuracy =  0.81288981289  ( 0.0639609981826 )\n",
      "iter num :37\n",
      "\tDecision Tree Accuracy =  0.769230769231  ( 0.0850925083279 )\n",
      "\tRandom Forest Tree Accuracy =  0.789473684211  ( 0.0681978968742 )\n",
      "\tLogistic Reg. Accuracy =  0.811740890688  ( 0.0634995430824 )\n",
      "iter num :38\n",
      "\tDecision Tree Accuracy =  0.771203155819  ( 0.0848699407256 )\n",
      "\tRandom Forest Tree Accuracy =  0.789940828402  ( 0.0673794510948 )\n",
      "\tLogistic Reg. Accuracy =  0.812623274162  ( 0.0629157313915 )\n",
      "iter num :39\n",
      "\tDecision Tree Accuracy =  0.772115384615  ( 0.0839957645452 )\n",
      "\tRandom Forest Tree Accuracy =  0.790384615385  ( 0.066589575743 )\n",
      "\tLogistic Reg. Accuracy =  0.814423076923  ( 0.0631328902738 )\n",
      "iter num :40\n",
      "\tDecision Tree Accuracy =  0.772983114447  ( 0.083146415783 )\n",
      "\tRandom Forest Tree Accuracy =  0.792682926829  ( 0.0673595612421 )\n",
      "\tLogistic Reg. Accuracy =  0.818011257036  ( 0.0663592464684 )\n",
      "iter num :41\n",
      "\tDecision Tree Accuracy =  0.771978021978  ( 0.0824023184889 )\n",
      "\tRandom Forest Tree Accuracy =  0.792124542125  ( 0.0666488020986 )\n",
      "\tLogistic Reg. Accuracy =  0.815934065934  ( 0.0668999761642 )\n",
      "iter num :42\n",
      "\tDecision Tree Accuracy =  0.772808586762  ( 0.0816162055064 )\n",
      "\tRandom Forest Tree Accuracy =  0.790697674419  ( 0.0665151786854 )\n",
      "\tLogistic Reg. Accuracy =  0.814847942755  ( 0.0664911182113 )\n",
      "iter num :43\n",
      "\tDecision Tree Accuracy =  0.773601398601  ( 0.080850736885 )\n",
      "\tRandom Forest Tree Accuracy =  0.789335664336  ( 0.0663587635547 )\n",
      "\tLogistic Reg. Accuracy =  0.813811188811  ( 0.0660818347991 )\n",
      "iter num :44\n",
      "\tDecision Tree Accuracy =  0.773504273504  ( 0.0799499441618 )\n",
      "\tRandom Forest Tree Accuracy =  0.788888888889  ( 0.0656841916473 )\n",
      "\tLogistic Reg. Accuracy =  0.81452991453  ( 0.0655171552025 )\n",
      "iter num :45\n",
      "\tDecision Tree Accuracy =  0.774247491639  ( 0.0792331621437 )\n",
      "\tRandom Forest Tree Accuracy =  0.788461538462  ( 0.0650295296166 )\n",
      "\tLogistic Reg. Accuracy =  0.813545150502  ( 0.0651369454969 )\n",
      "iter num :46\n",
      "\tDecision Tree Accuracy =  0.777414075286  ( 0.0812746907884 )\n",
      "\tRandom Forest Tree Accuracy =  0.791325695581  ( 0.0672028319727 )\n",
      "\tLogistic Reg. Accuracy =  0.815875613748  ( 0.0663504150593 )\n",
      "iter num :47\n",
      "\tDecision Tree Accuracy =  0.777243589744  ( 0.0804321159701 )\n",
      "\tRandom Forest Tree Accuracy =  0.791666666667  ( 0.0665401906796 )\n",
      "\tLogistic Reg. Accuracy =  0.815705128205  ( 0.0656660296428 )\n",
      "iter num :48\n",
      "\tDecision Tree Accuracy =  0.779434850863  ( 0.081041817891 )\n",
      "\tRandom Forest Tree Accuracy =  0.794348508634  ( 0.0684285548437 )\n",
      "\tLogistic Reg. Accuracy =  0.817111459969  ( 0.0657187933719 )\n",
      "iter num :49\n",
      "\tDecision Tree Accuracy =  0.779230769231  ( 0.0802400245418 )\n",
      "\tRandom Forest Tree Accuracy =  0.793846153846  ( 0.0678320236463 )\n",
      "\tLogistic Reg. Accuracy =  0.813846153846  ( 0.068956722557 )\n",
      "Decision Tree Accuracy =  0.779230769231  ( 0.0802400245418 )\n",
      "Random Forest Tree Accuracy =  0.793846153846  ( 0.0678320236463 )\n",
      "Logistic Reg. Accuracy =  0.813846153846  ( 0.068956722557 )\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy_score(Y_true, Y_predict):\n",
    "    c=0.\n",
    "    for i in range(len(Y_true)):\n",
    "        if Y_true[i]==Y_predict[i]:\n",
    "            c+=1\n",
    "    return c/len(Y_true)\n",
    "\n",
    "def evaluate_performance():\n",
    "    '''\n",
    "    Evaluate the performance of decision trees and logistic regression,\n",
    "    average over 1,000 trials of 10-fold cross validation\n",
    "\n",
    "    Return:\n",
    "      a matrix giving the performance that will contain the following entries:\n",
    "      stats[0,0] = mean accuracy of decision tree\n",
    "      stats[0,1] = std deviation of decision tree accuracy\n",
    "      stats[1,0] = mean accuracy of logistic regression\n",
    "      stats[1,1] = std deviation of logistic regression accuracy\n",
    "\n",
    "    ** Note that your implementation must follow this API**\n",
    "    '''\n",
    "\n",
    "    # Load Data\n",
    "    stats = np.zeros((3, 2))\n",
    "    filename = 'SPECTF.dat'\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    X = data[:, 1:]\n",
    "    y = np.array([data[:, 0]]).T\n",
    "    n, d = X.shape\n",
    "    num_folds=10\n",
    "    num_trials=50\n",
    "    all_accuracies=[]\n",
    "    rf_all_accuracies=[]\n",
    "    l_all_accuracies=[]\n",
    "    for trial in range(num_trials):\n",
    "        idx = np.arange(n)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        Xtrain = X[:n-n//num_folds, :]  # train on first 100 instances\n",
    "        Xtest = X[n-n//num_folds:, :]\n",
    "        ytrain = y[:n-n//num_folds, :]  # test on remaining instances\n",
    "        ytest = y[n-n//num_folds:, :]\n",
    "\n",
    "        # train the decision tree\n",
    "        classifier = DecisionTree(5)\n",
    "        classifier.fit(Xtrain, ytrain)\n",
    "        # output predictions on the remaining data\n",
    "        y_pred = classifier.predict(Xtest)\n",
    "        accuracy = accuracy_score(ytest, y_pred)\n",
    "        all_accuracies.append(accuracy)\n",
    "\n",
    "        rf_classifier=RandomForest(5,100,0.3)\n",
    "        rf_classifier.fit(Xtrain,ytrain)\n",
    "        rf_y_pred=rf_classifier.predict(Xtest)[0]\n",
    "        rf_accuracy=accuracy_score(ytest,rf_y_pred)\n",
    "        rf_all_accuracies.append(rf_accuracy)\n",
    "\n",
    "        l_classifier=logistic()\n",
    "        l_classifier.fit(Xtest,ytest)\n",
    "\n",
    "        l_y_pred=l_classifier.predict(Xtest)\n",
    "        l_accuracy=accuracy_score(ytest,l_y_pred)\n",
    "        l_all_accuracies.append(l_accuracy)\n",
    "        print('iter num :{0}'.format(trial))\n",
    "        \n",
    "        meanDecisionTreeAccuracy = np.mean(all_accuracies)\n",
    "        stddevDecisionTreeAccuracy = np.std(all_accuracies)\n",
    "        meanLogisticRegressionAccuracy = np.mean(l_all_accuracies)\n",
    "        stddevLogisticRegressionAccuracy = np.std(l_all_accuracies)\n",
    "        meanRandomForestAccuracy = np.mean(rf_all_accuracies)\n",
    "        stddevRandomForestAccuracy = np.std(rf_all_accuracies)\n",
    "\n",
    "    \n",
    "        stats[0, 0] = meanDecisionTreeAccuracy\n",
    "        stats[0, 1] = stddevDecisionTreeAccuracy\n",
    "        stats[1, 0] = meanRandomForestAccuracy\n",
    "        stats[1, 1] = stddevRandomForestAccuracy\n",
    "        stats[2, 0] = meanLogisticRegressionAccuracy\n",
    "        stats[2, 1] = stddevLogisticRegressionAccuracy\n",
    "        \n",
    "        \n",
    "        print (\"\\tDecision Tree Accuracy = \", stats[0, 0], \" (\", stats[0, 1], \")\" )\n",
    "        print (\"\\tRandom Forest Tree Accuracy = \", stats[1, 0], \" (\", stats[1, 1], \")\" )\n",
    "        print (\"\\tLogistic Reg. Accuracy = \", stats[2, 0], \" (\", stats[2, 1], \")\")\n",
    "\n",
    "    # compute the training accuracy of the model\n",
    "    meanDecisionTreeAccuracy = np.mean(all_accuracies)\n",
    "\n",
    "    stddevDecisionTreeAccuracy = np.std(all_accuracies)\n",
    "    meanLogisticRegressionAccuracy = np.mean(l_all_accuracies)\n",
    "    stddevLogisticRegressionAccuracy = np.std(l_all_accuracies)\n",
    "    meanRandomForestAccuracy = np.mean(rf_all_accuracies)\n",
    "    stddevRandomForestAccuracy = np.std(rf_all_accuracies)\n",
    "\n",
    "    # make certain that the return value matches the API specification\n",
    "    stats = np.zeros((3, 2))\n",
    "    stats[0, 0] = meanDecisionTreeAccuracy\n",
    "    stats[0, 1] = stddevDecisionTreeAccuracy\n",
    "    stats[1, 0] = meanRandomForestAccuracy\n",
    "    stats[1, 1] = stddevRandomForestAccuracy\n",
    "    stats[2, 0] = meanLogisticRegressionAccuracy\n",
    "    stats[2, 1] = stddevLogisticRegressionAccuracy\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Do not modify from HERE...\n",
    "if __name__ == \"__main__\":\n",
    "    stats = evaluate_performance()\n",
    "    print (\"Decision Tree Accuracy = \", stats[0, 0], \" (\", stats[0, 1], \")\" )\n",
    "    print (\"Random Forest Tree Accuracy = \", stats[1, 0], \" (\", stats[1, 1], \")\" )\n",
    "    print (\"Logistic Reg. Accuracy = \", stats[2, 0], \" (\", stats[2, 1], \")\")\n",
    "# ...to HERE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'SPECTF.dat'\n",
    "data = np.loadtxt(filename, delimiter=',')\n",
    "X = data[:, 1:]\n",
    "y = np.array([data[:, 0]]).T\n",
    "n, d = X.shape\n",
    "num_folds=10\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.arange(n)\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "Xtrain = X[:n-n//num_folds, :]  # train on first 100 instances\n",
    "Xtest = X[n-n//num_folds:, :]\n",
    "ytrain = y[:n-n//num_folds, :]  # test on remaining instances\n",
    "ytest = y[n-n//num_folds:, :]\n",
    "\n",
    "rf_classifier=RandomForest(15,100,0.3)\n",
    "rf_classifier.fit(Xtrain,ytrain)\n",
    "\n",
    "l_classifier=logistic()\n",
    "l_classifier.fit(Xtest,ytest)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[ 58.  63.  80.  71.  76.  70.  70.  71.  64.  63.  74.  78.  77.  75.  62.\n",
      "  61.  62.  56.  71.  52.  82.  71.  84.  85.  71.  71.  57.  47.  42.  39.\n",
      "  70.  70.  50.  70.  50.  46.  58.  60.  76.  73.  82.  77.  65.  66.] \n",
      "Y=[ 0.]\n"
     ]
    }
   ],
   "source": [
    "idx=-9\n",
    "print(\"X={0} \\nY={1}\".format(X[idx],y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0], [0.66666666666666663])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.predict([X[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_classifier.predict([X[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
